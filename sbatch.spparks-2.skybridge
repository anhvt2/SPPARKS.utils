#!/bin/bash
## Do not put any commands or blank lines before the #SBATCH lines
#SBATCH --nodes=1                     # Number of nodes - all cores per node are allocated to the job
#SBATCH --time=00:15:00               # Wall clock time (HH:MM:SS) - once the job exceeds this time, the job will be terminated (default is 5 minutes)
#SBATCH --account=FY140174	      # WC ID
#SBATCH --job-name=spk                # Name of job
#SBATCH --partition=batch             # partition/queue name: short or batch
                                      #            short: 4hrs wallclock limit
                                      #            batch: nodes reserved for > 4hrs (default)
#SBATCH --qos=normal                  # Quality of Service: long, large, priority or normal
                                      #           normal: request up to 48hrs wallclock (default)
                                      #           long:   request up to 96hrs wallclock and no larger than 64nodes
                                      #           large:  greater than 50% of cluster (special request)
                                      #           priority: High priority jobs (special request)


nodes=$SLURM_JOB_NUM_NODES           # Number of nodes - the number of nodes you have requested (for a list of SLURM environment variables see "man sbatch")
cores=4                             # Number MPI processes to run on each node (a.k.a. PPN)
                                     # CTS1 has 36 cores per node
# using openmpi-intel/1.10
# mpiexec --bind-to core --npernode $cores --n $(($cores*$nodes)) /path/to/executable [--args...]


# # #



seedId=$(date +%y%m%d%H%M%S)
# seedId=$(tr -dc '0-9' <<< $(basename $(pwd)))
# folderName="seed${seedId}"

inFile="in.grain_growth.spk"

# change in.spk
sed -i "11s|.*|seed		${seedId}|" $inFile

echo "SPK INPUT SCRIPT:"
cat $inFile; echo; echo; 
mpiexec --bind-to core --npernode $cores --n $(($cores*$nodes)) spk < $inFile
# mpirun -np 4 spk < $inFile

#rm -v !("grain_growth_Bnw.200.jpg")
folderName=$(pwd | rev | cut -d/ -f1 | rev)
mv grain_growth_Bnw.200.jpg gg_Sample_${folderName}.jpg
rm -f grain_growth*.jpg
rm -f in*spk *cluster *out log.spparks
